{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pylab as plt\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Analysing the graph of hierarcy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#visualize graph\n",
    "def save_graph(graph,file_name):\n",
    "    #initialze Figure\n",
    "    plt.figure(num=None, figsize=(20, 20), dpi=80)\n",
    "    plt.axis('off')\n",
    "    fig = plt.figure(1)\n",
    "    pos = nx.spring_layout(graph)\n",
    "    nx.draw_networkx_nodes(graph,pos)\n",
    "    nx.draw_networkx_edges(graph,pos)\n",
    "\n",
    "    cut = 1.00\n",
    "    xmax = cut * max(xx for xx, yy in pos.values())\n",
    "    ymax = cut * max(yy for xx, yy in pos.values())\n",
    "    plt.xlim(0, xmax)\n",
    "    plt.ylim(0, ymax)\n",
    "\n",
    "    plt.savefig(file_name,bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    del fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###This exercise is to remove a part of hierarchy to make data managable for my system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create directed graph from hierarchy\n",
    "def create_digraph(file):\n",
    "    graph = nx.DiGraph()\n",
    "\n",
    "    file = open(file)\n",
    "\n",
    "    lines = file.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        edge = line.rstrip().split(' ')\n",
    "        graph.add_edge(int(edge[0]), int(edge[1]))\n",
    "    file.close()\n",
    "    return graph\n",
    "\n",
    "graph = create_digraph('hierarchy.txt')\n",
    "all_nodes = nx.nodes(graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478020\n",
      "250000\n",
      "228020\n"
     ]
    }
   ],
   "source": [
    "print(len(all_nodes))\n",
    "nodes_to_remove = all_nodes[0:250000]\n",
    "print(len(nodes_to_remove))\n",
    "graph.remove_nodes_from(nodes_to_remove)\n",
    "print(graph.number_of_nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###After removing some nodes from the graph we now look for largest connected component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147739\n"
     ]
    }
   ],
   "source": [
    "#find a connected component of graph with maximum number of nodes\n",
    "def single_connected_graph(graph):\n",
    "    new_graphs = nx.weakly_connected_component_subgraphs(graph)\n",
    "\n",
    "    the_graphs = []\n",
    "\n",
    "    for g in new_graphs:\n",
    "        the_graphs.append(g)\n",
    "    max_nodes = 0\n",
    "    max_nodes_index = 0\n",
    "    index = 0\n",
    "    for g in the_graphs:\n",
    "        num_nodes = g.number_of_nodes()\n",
    "        if(num_nodes > max_nodes):\n",
    "            max_nodes = num_nodes\n",
    "            max_nodes_index = index\n",
    "        index += 1\n",
    "    final_graph = the_graphs[max_nodes_index]\n",
    "    return final_graph\n",
    "\n",
    "final_graph = single_connected_graph(graph)\n",
    "final_nodes = nx.nodes(final_graph)\n",
    "print(len(final_nodes))\n",
    "# nx.write_edgelist(final_graph, \"new_hierarchy.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####These are the nodes in a subset of hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330281\n",
      "[1048576, 1048577, 2, 174763, 4, 2097157, 6, 1, 8, 2097161]\n"
     ]
    }
   ],
   "source": [
    "discarded_nodes = []\n",
    "map_final_nodes = {}\n",
    "\n",
    "for node in final_nodes:\n",
    "    map_final_nodes[node] = 1\n",
    "\n",
    "for node in all_nodes:\n",
    "    if(node not in map_final_nodes):\n",
    "        discarded_nodes.append(node)\n",
    "    \n",
    "map_discarded_nodes = {}\n",
    "\n",
    "for node in discarded_nodes:\n",
    "    map_discarded_nodes[node] = 1\n",
    "print(len(discarded_nodes))\n",
    "print(discarded_nodes[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we filter our data to retain only the data with nodes in our retained hierarchy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = open('train-remapped.csv', 'r')\n",
    "output_file = open('final_training.csv', 'w')\n",
    "lines = data.readlines()\n",
    "output_file.write(lines[0])\n",
    "for i in range(1,len(lines)):\n",
    "#     print(lines[i])\n",
    "    classes = lines[i].rstrip().split(\":\")[0]\n",
    "    true_classes = []\n",
    "    for j in range(len(classes)):\n",
    "        index = len(classes) -j - 1\n",
    "        if(classes[index] == ' '):\n",
    "            true_classes = classes[0:index].split(',')\n",
    "            break\n",
    "    flag = False\n",
    "    for c in true_classes:\n",
    "        if int(c) in map_discarded_nodes:\n",
    "            flag = True\n",
    "            break\n",
    "    if(not flag):\n",
    "        output_file.write(lines[i])\n",
    "    \n",
    "    \n",
    "data.close()\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2365437\n",
      "216313\n"
     ]
    }
   ],
   "source": [
    "print(len(lines))\n",
    "\n",
    "output = open('final_training.csv', 'r')\n",
    "print(len(output.readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.is_directed_acyclic_graph(final_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###It turns out that even the reduced hierarchy is not a DAG. This is problamatic for any meaningful traversal through the heirarchy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Another way of looking at the data would be looking at most recurring classes and those should be the classes we may focus on first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#count number of examples for each class\n",
    "import operator\n",
    "def count_concepts(file_name):\n",
    "    file = open(file_name, 'r')\n",
    "    lines = file.readlines()\n",
    "    class_map ={}\n",
    "    for i in range(1, len(lines)):\n",
    "        classes = lines[i].rstrip().split(\":\")[0]\n",
    "        true_classes = []\n",
    "        for j in range(len(classes)):\n",
    "            index = len(classes) -j - 1\n",
    "            if(classes[index] == ' '):\n",
    "                true_classes = classes[0:index].split(',')\n",
    "                break\n",
    "        for c in true_classes:\n",
    "            if int(c) in class_map:\n",
    "                class_map[int(c)] = class_map[int(c)] + 1\n",
    "            else:\n",
    "                class_map[int(c)] = 1\n",
    "    file.close()\n",
    "    return class_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(24177, 387168), (285613, 41104), (98808, 14838), (264962, 12556), (167593, 11400), (242532, 10435), (52954, 10026), (300558, 9473), (444502, 9217), (78249, 9161)]\n",
      "[(445683, 1), (445690, 1), (445691, 1), (445699, 1), (445707, 1), (445710, 1), (445717, 1), (445722, 1), (445724, 1)]\n",
      "43368\n"
     ]
    }
   ],
   "source": [
    "class_map = count_concepts('train-remapped.csv')\n",
    "\n",
    "sorted_list = sorted(class_map.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "print(sorted_list[0:10])\n",
    "print(sorted_list[-10:-1])\n",
    "print(len(filter(lambda x: x[1] == 1, sorted_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some classes are dispropotionately popular while a lot (43368) have only 1 sample available. So we look for classes which have got some reasonable number of sample available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24177, 285613, 98808, 264962, 167593, 242532, 52954]\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "#Find all classes with atleast 1000 examples in dataset\n",
    "relevant_classes = list(map(lambda x: x[0], list(filter(lambda x: x[1] >= 10000, sorted_list))))\n",
    "print(relevant_classes)\n",
    "print(len(relevant_classes))\n",
    "\n",
    "relevant_classes_map = {}\n",
    "\n",
    "for c in relevant_classes:\n",
    "    relevant_classes_map[c] = 1\n",
    "# print(relevant_classes_map.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we consider classes which have atleast 10000 samples then we get onl 7 classes in case of atleast 1000 samples likewise there are around 350 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hfile = open('hierarchy.txt', 'r')\n",
    "newhfile = open('updated_hierarchy.txt', 'w')\n",
    "for line in hfile.readlines():\n",
    "    x = line.rstrip().split(' ')\n",
    "    \n",
    "    if(int(x[0]) in relevant_classes_map or int(x[1]) in relevant_classes_map):\n",
    "        newhfile.write(line)\n",
    "hfile.close()\n",
    "newhfile.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train_file = open('train-remapped.csv', 'r')\n",
    "# update_train_file = open('train-updated.csv', 'w')\n",
    "# lines = train_file.readlines()\n",
    "# # update_train_file.write(lines[0])\n",
    "# for i in range(1, len(lines)):\n",
    "#     classes = lines[i].rstrip().split(\":\")[0]\n",
    "#     true_classes = []\n",
    "#     for j in range(len(classes)):\n",
    "#         index = len(classes) -j - 1\n",
    "#         if(classes[index] == ' '):\n",
    "#             true_classes = classes[0:index].split(',')\n",
    "#             break\n",
    "# #     for c in true_classes:\n",
    "# #         if int(c) in relevant_classes_map:\n",
    "#     new_classes = get_new_classes(true_classese)\n",
    "#     if(len(true_classes) > 0):\n",
    "#         update_train_file.write(lines[i])\n",
    "# #     break\n",
    "# train_file.close()\n",
    "# update_train_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "723085\n"
     ]
    }
   ],
   "source": [
    "update_train_file = open('train-updated.csv', 'r')\n",
    "print(len(update_train_file.readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_new_classes(classes):\n",
    "    new_classes = []\n",
    "    for c in classes:\n",
    "        if c in relevant_classes_map:\n",
    "            new_classes.append(c)\n",
    "    return list(map(str, new_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train_file = open('train-remapped.csv', 'r')\n",
    "# update_train_file = open('train-updated-small.csv', 'w')\n",
    "# lines = train_file.readlines()\n",
    "# # update_train_file.write(lines[0])\n",
    "# for i in range(1, len(lines)):\n",
    "#     classes = lines[i].rstrip().split(\":\")[0]\n",
    "#     features = ''\n",
    "#     true_classes = []\n",
    "#     for j in range(len(classes)):\n",
    "#         index = len(classes) -j - 1\n",
    "#         if(classes[index] == ' '):\n",
    "#             true_classes = list(map(int, classes[0:index].split(',')))\n",
    "#             features = lines[i].split(classes[0:index])[1]\n",
    "#             break\n",
    "# #     for c in true_classes:\n",
    "# #         if int(c) in relevant_classes_map:\n",
    "\n",
    "#     new_classes = get_new_classes(true_classes)\n",
    "# #     print(new_classes)\n",
    "# #     print(features)\n",
    "#     if(len(new_classes) > 0):\n",
    "#         update_train_file.write(str(\",\".join(new_classes)) + features)\n",
    "# #     break\n",
    "# train_file.close()\n",
    "# update_train_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###This method is for measuring recall and  mistake(when a sample is assigned only one class and it is not among any valid class for the sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def multilabel_performance(pred, true, indexes):\n",
    "    \n",
    "    spliced_indexes  = []\n",
    "    previous_index = indexes[0]\n",
    "    temp_indexes = []\n",
    "    total_score = 0\n",
    "    total_mistake = 0\n",
    "    for i in range(len(indexes)):\n",
    "        if(indexes[i] != previous_index):\n",
    "            spliced_indexes.append(temp_indexes)\n",
    "            temp_indexes = []\n",
    "        temp_indexes.append(i)\n",
    "    spliced_indexes.append(temp_indexes)\n",
    "    total_samples = len(spliced_indexes)\n",
    "    for same_index  in spliced_indexes:\n",
    "        temp_score = 0\n",
    "        for x in same_index:\n",
    "            if(pred[x] == true[x]):\n",
    "                temp_score = 1\n",
    "        if(temp_score == 1):\n",
    "            total_score += 1\n",
    "            temp_score = 0\n",
    "        else:\n",
    "            total_mistake += 1\n",
    "    return float(total_score)/total_samples, float(total_mistake)/total_samples\n",
    "            \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Here we convert the data from multilabel to single label for easy manipulation. We also retain their indexes in case a sample falls in more than one class. These indexes will be required while measuring accuracy of prediction. When a sample has multiple labels we create a separate row for it with each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_file = open('train-remapped.csv', 'r')\n",
    "update_train_file = open('train-updated-single-label-small.csv', 'w')\n",
    "index_file = open('index.csv', 'w')\n",
    "lines = train_file.readlines()\n",
    "# update_train_file.write(lines[0])\n",
    "k = 0\n",
    "for i in range(1, len(lines)):\n",
    "    classes = lines[i].rstrip().split(\":\")[0]\n",
    "    features = ''\n",
    "    true_classes = []\n",
    "    for j in range(len(classes)):\n",
    "        index = len(classes) -j - 1\n",
    "        if(classes[index] == ' '):\n",
    "            true_classes = list(map(int, classes[0:index].split(',')))\n",
    "            features = lines[i].split(classes[0:index])[1]\n",
    "            break\n",
    "#     for c in true_classes:\n",
    "#         if int(c) in relevant_classes_map:\n",
    "\n",
    "    new_classes = get_new_classes(true_classes)\n",
    "#     print(new_classes)\n",
    "#     print(features)\n",
    "    flag = False\n",
    "    for c in new_classes:\n",
    "        update_train_file.write(c + features)\n",
    "        index_file.write(str(k) + '\\n')\n",
    "        flag = True\n",
    "    if(flag):\n",
    "        k += 1\n",
    "        flag = False\n",
    "        \n",
    "#     break\n",
    "train_file.close()\n",
    "update_train_file.close()\n",
    "index_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "import sklearn\n",
    "import mdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataf = load_svmlight_file(\"train-updated-single-label-small.csv\")\n",
    "indexes = []\n",
    "\n",
    "with open('index.csv', 'r') as f:\n",
    "    for x in f.readlines():\n",
    "        indexes.append(int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "487527\n"
     ]
    }
   ],
   "source": [
    "train_all = dataf[0]\n",
    "test_all = dataf[1]\n",
    "train_samples = 100000\n",
    "test_samples = 100000\n",
    "X_train = dataf[0][0:train_samples]\n",
    "y_train = dataf[1][0:train_samples]\n",
    "indexes_train = indexes[0:train_samples]\n",
    "X_test = dataf[0][train_samples:train_samples + test_samples]\n",
    "y_test = dataf[1][train_samples:train_samples + test_samples]\n",
    "indexes_test = indexes[train_samples:train_samples + test_samples]\n",
    "print(len(indexes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Testing SVM classification on a small sample and see how it performs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = sklearn.svm.SVC(verbose=3)\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[76790  1000  3739  1910  4811  5488  6262]\n",
      " [    0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0]]\n",
      "0.7679\n",
      "(0.7679, 0.2321)\n",
      "****************************************************************************************************\n",
      "[[78695  1904  2777  1864  2318  2634  9808]\n",
      " [    0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0]]\n",
      "0.78695\n",
      "(0.78695, 0.21305)\n"
     ]
    }
   ],
   "source": [
    "train_pred = model.predict(X_train)\n",
    "\n",
    "\n",
    "print(sklearn.metrics.confusion_matrix(train_pred, y_train))\n",
    "print(sklearn.metrics.accuracy_score(train_pred, y_train))\n",
    "print(multilabel_performance(train_pred, y_train, indexes_train))\n",
    "\n",
    "print(\"*\" * 100)\n",
    "test_pred = model.predict(X_test)\n",
    "\n",
    "print(sklearn.metrics.confusion_matrix(test_pred, y_test))\n",
    "print(sklearn.metrics.accuracy_score(test_pred, y_test))\n",
    "print(multilabel_performance(test_pred, y_test, indexes_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Using svm to train a model. The model is very simple it predicts every sample as the most popular class. In doing so it may reach around .787 recall. To do better lets use a simple decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            random_state=None, splitter='best')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeldecisionTree = sklearn.tree.DecisionTreeClassifier()\n",
    "modeldecisionTree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[76790   800     0     0  3880  3882  6261]\n",
      " [    0   200     0     0     0     0     0]\n",
      " [    0     0  3739  1425     0     0     0]\n",
      " [    0     0     0   485     0     0     0]\n",
      " [    0     0     0     0   931   835     0]\n",
      " [    0     0     0     0     0   771     0]\n",
      " [    0     0     0     0     0     0     1]]\n",
      "0.82917\n",
      "(0.82917, 0.17083)\n",
      "****************************************************************************************************\n",
      "[[77879  1717   339   224  2013  2037  9542]\n",
      " [   53   186     0     0     0     0     3]\n",
      " [  525     0  2266  1466    16    51   202]\n",
      " [   31     0   160   169     2     5     7]\n",
      " [  118     0     7     3   226   288    39]\n",
      " [   87     1     5     2    61   253    15]\n",
      " [    2     0     0     0     0     0     0]]\n",
      "0.80979\n",
      "(0.80979, 0.19021)\n"
     ]
    }
   ],
   "source": [
    "train_pred = modeldecisionTree.predict(X_train)\n",
    "print(sklearn.metrics.confusion_matrix(train_pred, y_train))\n",
    "print(sklearn.metrics.accuracy_score(train_pred, y_train))\n",
    "print(multilabel_performance(train_pred, y_train, indexes_train))\n",
    "\n",
    "\n",
    "print(\"*\" * 100)\n",
    "test_pred = modeldecisionTree.predict(X_test)\n",
    "print(sklearn.metrics.confusion_matrix(test_pred, y_test))\n",
    "print(sklearn.metrics.accuracy_score(test_pred, y_test))\n",
    "print(multilabel_performance(test_pred, y_test, indexes_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###With decision tree there is improvement. Shifting of prediction towards diagonal is observed. When base line is that good much improvement is difficult. There is not much overfitting too as train and test performance is close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Lets filter not so important attributes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def attributes(file):\n",
    "    attribute_map = {}\n",
    "    with open(file) as f:\n",
    "        for sample in f.readlines():\n",
    "            attributes = sample.split(\" \")[1:]\n",
    "            for attribute in attributes:\n",
    "                a = int(attribute.split(\":\")[0])\n",
    "                if a in attribute_map:\n",
    "                    attribute_map[a] += 1\n",
    "                else:\n",
    "                    attribute_map[a] = 1\n",
    "    sorted_list = sorted(attribute_map.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sorted_list\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "attributes_frequency = attributes(\"train-updated-single-label-small.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(701, 268273), (715, 139185), (232, 124295), (695, 90083), (1144, 89194), (45, 80801), (649, 79973), (605, 77205), (229, 76998), (389, 72781)]\n",
      "[(1048484, 1), (1048485, 1), (1048486, 1), (1048488, 1), (1048489, 1), (1048503, 1), (1048505, 1), (1048506, 1), (1048541, 1)]\n",
      "(487527, 1617874)\n",
      "430310\n",
      "210231\n",
      "429501\n"
     ]
    }
   ],
   "source": [
    "print(attributes_frequency[0:10])\n",
    "print(attributes_frequency[-10:-1])\n",
    "\n",
    "print(np.shape(train_all))\n",
    "print(len(attributes_frequency))\n",
    "print(len(filter(lambda x: x[1] == 1, attributes_frequency)))\n",
    "print(len(filter(lambda x: x[1] < 5000, attributes_frequency)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By looking at the frequency of attributes used in data we observe that around half of the attributes occur only once throughout the data. We can filter out these attributes to reduce dimention although the correct cutoff should be determined experimentally.Here we just make a resonable cutoff of 5000 as the class with lowest number of samples has slightly more samples than 10000. Nonetheless we should remember if we consider all the samples corrosponding to all classes then a lot of these discarded attributes may become important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "809\n"
     ]
    }
   ],
   "source": [
    "useful_attributes = list(filter(lambda x: x[1] >= 5000, attributes_frequency))\n",
    "print(len(useful_attributes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###809 attributes are still quite a number to work with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Now lets map these attributes from their origin index to 0-808"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mapped_attributes = {}\n",
    "for i in range(len(useful_attributes)):\n",
    "    mapped_attributes[useful_attributes[i][0]] = i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Now lets write the data with remapped attribute in another file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"train-updated-single-label-small.csv\") as inputfile:\n",
    "    with open(\"train-reduced.csv\", 'w') as outputfile:\n",
    "        for line in inputfile.readlines():\n",
    "            data = line.split(\" \")\n",
    "            new_data = []\n",
    "            new_data.append(data[0])\n",
    "            for i in range(1, len(data)):\n",
    "                k = data[i].split(\":\")[0]\n",
    "                v = data[i].split(\":\")[1]\n",
    "                if(int(k) in mapped_attributes):\n",
    "                    new_data.append(\":\".join([str(mapped_attributes[int(k)]), v]))\n",
    "            if(len(new_data) > 1):\n",
    "                outputfile.write(\" \".join(new_data) + '\\n')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataf = load_svmlight_file(\"train-updated-single-label-small.csv\")\n",
    "indexes = []\n",
    "\n",
    "with open('index.csv', 'r') as f:\n",
    "    for x in f.readlines():\n",
    "        indexes.append(int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "487527\n",
      "(487527,)\n"
     ]
    }
   ],
   "source": [
    "train_all = dataf[0]\n",
    "test_all = dataf[1]\n",
    "train_samples = 300000\n",
    "test_samples = 180000\n",
    "X_train = dataf[0][0:train_samples]\n",
    "y_train = dataf[1][0:train_samples]\n",
    "indexes_train = indexes[0:train_samples]\n",
    "X_test = dataf[0][train_samples:train_samples + test_samples]\n",
    "y_test = dataf[1][train_samples:train_samples + test_samples]\n",
    "indexes_test = indexes[train_samples:train_samples + test_samples]\n",
    "print(len(indexes))\n",
    "print(np.shape(dataf[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[234486   3854      0      0   7132   6977  25698]\n",
      " [     0   1096      0      0      0      0      0]\n",
      " [     0      0   9627   4587      0      0      0]\n",
      " [     0      0      0   1843      0      0      0]\n",
      " [     0      0      0      0   1532   1295      0]\n",
      " [     0      0      0      0      0   1868      0]\n",
      " [     0      0      0      0      0      0      5]]\n",
      "0.834856666667\n",
      "(0.8348566666666667, 0.16514333333333334)\n",
      "****************************************************************************************************\n",
      "[[145381   3453    403    286   1433   1553  14898]\n",
      " [   139   1450      0      0      0      0     10]\n",
      " [   513      0   4265   3769     19     28    173]\n",
      " [    36      0    265    648      1      1     10]\n",
      " [    96      2      2      1    182    253     28]\n",
      " [   104      0      8     13     71    493     12]\n",
      " [     1      0      0      0      0      0      0]]\n",
      "0.846772222222\n",
      "(0.8467722222222223, 0.15322777777777777)\n"
     ]
    }
   ],
   "source": [
    "modeldecisionTree = sklearn.tree.DecisionTreeClassifier()\n",
    "modeldecisionTree.fit(X_train, y_train)\n",
    "train_pred = modeldecisionTree.predict(X_train)\n",
    "print(sklearn.metrics.confusion_matrix(train_pred, y_train))\n",
    "print(sklearn.metrics.accuracy_score(train_pred, y_train))\n",
    "print(multilabel_performance(train_pred, y_train, indexes_train))\n",
    "\n",
    "\n",
    "print(\"*\" * 100)\n",
    "test_pred = modeldecisionTree.predict(X_test)\n",
    "print(sklearn.metrics.confusion_matrix(test_pred, y_test))\n",
    "print(sklearn.metrics.accuracy_score(test_pred, y_test))\n",
    "print(multilabel_performance(test_pred, y_test, indexes_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###With reduced dimentions we are able to train on more data efficiently and consequently we get better performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
