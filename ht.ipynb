{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pylab as plt\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#visualize graph\n",
    "def save_graph(graph,file_name):\n",
    "    #initialze Figure\n",
    "    plt.figure(num=None, figsize=(20, 20), dpi=80)\n",
    "    plt.axis('off')\n",
    "    fig = plt.figure(1)\n",
    "    pos = nx.spring_layout(graph)\n",
    "    nx.draw_networkx_nodes(graph,pos)\n",
    "    nx.draw_networkx_edges(graph,pos)\n",
    "\n",
    "    cut = 1.00\n",
    "    xmax = cut * max(xx for xx, yy in pos.values())\n",
    "    ymax = cut * max(yy for xx, yy in pos.values())\n",
    "    plt.xlim(0, xmax)\n",
    "    plt.ylim(0, ymax)\n",
    "\n",
    "    plt.savefig(file_name,bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    del fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create directed graph from hierarchy\n",
    "def create_digraph(file):\n",
    "    graph = nx.DiGraph()\n",
    "\n",
    "    file = open(file)\n",
    "\n",
    "    lines = file.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        edge = line.rstrip().split(' ')\n",
    "        graph.add_edge(int(edge[0]), int(edge[1]))\n",
    "    file.close()\n",
    "    return graph\n",
    "\n",
    "graph = create_digraph('hierarchy.txt')\n",
    "all_nodes = nx.nodes(graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478020\n",
      "250000\n",
      "228020\n"
     ]
    }
   ],
   "source": [
    "print(len(all_nodes))\n",
    "nodes_to_remove = all_nodes[0:250000]\n",
    "print(len(nodes_to_remove))\n",
    "graph.remove_nodes_from(nodes_to_remove)\n",
    "print(graph.number_of_nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147739\n"
     ]
    }
   ],
   "source": [
    "#find a connected component of graph with maximum number of nodes\n",
    "def single_connected_graph(graph):\n",
    "    new_graphs = nx.weakly_connected_component_subgraphs(graph)\n",
    "\n",
    "    the_graphs = []\n",
    "\n",
    "    for g in new_graphs:\n",
    "        the_graphs.append(g)\n",
    "    max_nodes = 0\n",
    "    max_nodes_index = 0\n",
    "    index = 0\n",
    "    for g in the_graphs:\n",
    "        num_nodes = g.number_of_nodes()\n",
    "        if(num_nodes > max_nodes):\n",
    "            max_nodes = num_nodes\n",
    "            max_nodes_index = index\n",
    "        index += 1\n",
    "    final_graph = the_graphs[max_nodes_index]\n",
    "    return final_graph\n",
    "\n",
    "final_graph = single_connected_graph(graph)\n",
    "final_nodes = nx.nodes(final_graph)\n",
    "print(len(final_nodes))\n",
    "# nx.write_edgelist(final_graph, \"new_hierarchy.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330281\n",
      "[1048576, 1048577, 2, 174763, 4, 2097157, 6, 1, 8, 2097161]\n"
     ]
    }
   ],
   "source": [
    "discarded_nodes = []\n",
    "map_final_nodes = {}\n",
    "\n",
    "for node in final_nodes:\n",
    "    map_final_nodes[node] = 1\n",
    "\n",
    "for node in all_nodes:\n",
    "    if(node not in map_final_nodes):\n",
    "        discarded_nodes.append(node)\n",
    "    \n",
    "map_discarded_nodes = {}\n",
    "\n",
    "for node in discarded_nodes:\n",
    "    map_discarded_nodes[node] = 1\n",
    "print(len(discarded_nodes))\n",
    "print(discarded_nodes[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = open('train-remapped.csv', 'r')\n",
    "output_file = open('final_training.csv', 'w')\n",
    "lines = data.readlines()\n",
    "output_file.write(lines[0])\n",
    "for i in range(1,len(lines)):\n",
    "#     print(lines[i])\n",
    "    classes = lines[i].rstrip().split(\":\")[0]\n",
    "    true_classes = []\n",
    "    for j in range(len(classes)):\n",
    "        index = len(classes) -j - 1\n",
    "        if(classes[index] == ' '):\n",
    "            true_classes = classes[0:index].split(',')\n",
    "            break\n",
    "    flag = False\n",
    "    for c in true_classes:\n",
    "        if int(c) in map_discarded_nodes:\n",
    "            flag = True\n",
    "            break\n",
    "    if(not flag):\n",
    "        output_file.write(lines[i])\n",
    "    \n",
    "    \n",
    "data.close()\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2365437\n",
      "216313\n"
     ]
    }
   ],
   "source": [
    "print(len(lines))\n",
    "\n",
    "output = open('final_training.csv', 'r')\n",
    "print(len(output.readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.is_directed_acyclic_graph(final_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#count number of examples for each class\n",
    "import operator\n",
    "def count_concepts(file_name):\n",
    "    file = open(file_name, 'r')\n",
    "    lines = file.readlines()\n",
    "    class_map ={}\n",
    "    for i in range(1, len(lines)):\n",
    "        classes = lines[i].rstrip().split(\":\")[0]\n",
    "        true_classes = []\n",
    "        for j in range(len(classes)):\n",
    "            index = len(classes) -j - 1\n",
    "            if(classes[index] == ' '):\n",
    "                true_classes = classes[0:index].split(',')\n",
    "                break\n",
    "        for c in true_classes:\n",
    "            if int(c) in class_map:\n",
    "                class_map[int(c)] = class_map[int(c)] + 1\n",
    "            else:\n",
    "                class_map[int(c)] = 1\n",
    "    file.close()\n",
    "    return class_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(24177, 387168), (285613, 41104), (98808, 14838), (264962, 12556), (167593, 11400), (242532, 10435), (52954, 10026), (300558, 9473), (444502, 9217), (78249, 9161)]\n"
     ]
    }
   ],
   "source": [
    "class_map = count_concepts('train-remapped.csv')\n",
    "\n",
    "sorted_list = sorted(class_map.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "print(sorted_list[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24177, 285613, 98808, 264962, 167593, 242532, 52954, 300558, 444502, 78249, 237290, 220514, 10721, 337728, 174545, 73518, 24016, 327590, 154064, 374771, 366417, 87241, 73092, 115838, 334220, 169902, 59758, 347803, 364106, 178462, 287120, 14843, 260304, 73462, 23611, 322170, 174425, 167844, 29462, 158599, 299629, 34161, 390974, 228232, 150636, 341276, 36224, 289559, 418360, 323972, 352578, 284433, 383600, 300073, 231746, 60639, 251484, 2830, 183203, 234578, 283823, 161537, 286264, 304661, 93718, 348488, 139391, 397350, 244711, 186125, 419276, 1508, 398319, 428719, 290537, 403132, 395447, 351111, 324660, 13252, 131804, 430081, 24052, 244616, 86836, 393137, 374859, 111772, 206933, 109127, 96443, 228238, 269785, 2903, 272741, 213350, 225356, 174595, 414726, 429208, 151184, 20627, 259458, 97284, 143799, 316670, 1859, 89192, 93043, 165833, 198076, 198336, 363791, 118798, 396560, 402991, 439461, 145157, 171670, 24513, 372228, 234240, 212458, 98638, 251951, 411448, 258850, 65676, 390846, 354758, 103932, 332072, 318717, 236790, 82093, 11853, 364054, 191198, 380055, 131885, 263237, 387666, 92298, 308993, 438906, 90557, 128420, 10211, 133029, 443886, 99410, 125304, 102470, 134458, 75435, 300270, 348319, 24823, 310782, 290646, 343977, 134141, 113446, 15130, 89865, 95504, 317914, 114873, 46195, 18239, 433190, 22903, 86180, 61229, 50104, 224092, 155422, 256076, 2505, 255523, 201870, 303823, 195420, 104651, 78599, 34572, 266544, 289056, 95308, 112236, 257731, 162168, 35366, 362472, 244516, 212432, 142148, 275465, 384343, 414131, 8018, 302794, 253575, 76004, 217105, 76866, 35073, 66044, 187981, 324931, 52349, 79561, 354791, 395825, 89863, 237392, 368542, 68599, 219058, 619, 235345, 354381, 260326, 131834, 30755, 238597, 84330, 114538, 251688, 77779, 74285, 123514, 178176, 141772, 185999, 51347, 82615, 443325, 27398, 318659, 380032, 154165, 413981, 221836, 333275, 91533, 402678, 293473, 173668, 400434, 401421, 435440, 265103, 417730, 149779, 287522, 48448, 434112, 132942, 408771, 221046, 111370, 228308, 238882, 312330, 423304, 144045, 246728, 307819, 321123, 439537, 42459, 236518, 272199, 376516, 368878, 181774, 438913, 209204, 78945, 282584, 317854, 440234, 324166, 171930, 340611, 5007, 231362, 394091, 389616, 117362, 156349, 223841, 79464, 256107, 114277, 386699, 119346, 229071, 312123, 440847, 265442, 118149, 264219, 406958, 322778, 135268, 309149, 88179, 166150, 379896, 8746, 205211, 10846, 242002, 80175, 113949, 354187, 329853, 374741, 439993, 282661, 389945, 10088, 201320, 242897, 174849, 175715, 293686, 419967, 255550, 227720, 42540, 336752, 441809, 118483, 271868, 377749, 229765, 273052, 286463, 104391, 155538, 162010, 81677, 268358, 303407, 30990, 116515, 191146, 47471, 395999, 321692, 28656, 47886, 141345, 290102, 230599, 14661, 442079, 101880, 281215, 381201, 216249, 27560, 336745]\n",
      "366\n"
     ]
    }
   ],
   "source": [
    "#Find all classes with atleast 1000 examples in dataset\n",
    "relevant_classes = list(map(lambda x: x[0], list(filter(lambda x: x[1] >= 1000, sorted_list))))\n",
    "print(relevant_classes)\n",
    "print(len(relevant_classes))\n",
    "\n",
    "relevant_classes_map = {}\n",
    "\n",
    "for c in relevant_classes:\n",
    "    relevant_classes_map[c] = 1\n",
    "# print(relevant_classes_map.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hfile = open('hierarchy.txt', 'r')\n",
    "newhfile = open('updated_hierarchy.txt', 'w')\n",
    "for line in hfile.readlines():\n",
    "    x = line.rstrip().split(' ')\n",
    "    \n",
    "    if(int(x[0]) in relevant_classes_map or int(x[1]) in relevant_classes_map):\n",
    "        newhfile.write(line)\n",
    "hfile.close()\n",
    "newhfile.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train_file = open('train-remapped.csv', 'r')\n",
    "# update_train_file = open('train-updated.csv', 'w')\n",
    "# lines = train_file.readlines()\n",
    "# # update_train_file.write(lines[0])\n",
    "# for i in range(1, len(lines)):\n",
    "#     classes = lines[i].rstrip().split(\":\")[0]\n",
    "#     true_classes = []\n",
    "#     for j in range(len(classes)):\n",
    "#         index = len(classes) -j - 1\n",
    "#         if(classes[index] == ' '):\n",
    "#             true_classes = classes[0:index].split(',')\n",
    "#             break\n",
    "# #     for c in true_classes:\n",
    "# #         if int(c) in relevant_classes_map:\n",
    "#     new_classes = get_new_classes(true_classese)\n",
    "#     if(len(true_classes) > 0):\n",
    "#         update_train_file.write(lines[i])\n",
    "# #     break\n",
    "# train_file.close()\n",
    "# update_train_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "723085\n"
     ]
    }
   ],
   "source": [
    "update_train_file = open('train-updated.csv', 'r')\n",
    "print(len(update_train_file.readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_new_classes(classes):\n",
    "    new_classes = []\n",
    "    for c in classes:\n",
    "        if c in relevant_classes_map:\n",
    "            new_classes.append(c)\n",
    "    return list(map(str, new_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_file = open('train-remapped.csv', 'r')\n",
    "update_train_file = open('train-updated.csv', 'w')\n",
    "lines = train_file.readlines()\n",
    "# update_train_file.write(lines[0])\n",
    "for i in range(1, len(lines)):\n",
    "    classes = lines[i].rstrip().split(\":\")[0]\n",
    "    features = ''\n",
    "    true_classes = []\n",
    "    for j in range(len(classes)):\n",
    "        index = len(classes) -j - 1\n",
    "        if(classes[index] == ' '):\n",
    "            true_classes = list(map(int, classes[0:index].split(',')))\n",
    "            features = lines[i].split(classes[0:index])[1]\n",
    "            break\n",
    "#     for c in true_classes:\n",
    "#         if int(c) in relevant_classes_map:\n",
    "\n",
    "    new_classes = get_new_classes(true_classes)\n",
    "#     print(new_classes)\n",
    "#     print(features)\n",
    "    if(len(new_classes) > 0):\n",
    "        update_train_file.write(str(\",\".join(new_classes)) + features)\n",
    "#     break\n",
    "train_file.close()\n",
    "update_train_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_file = open('train-remapped.csv', 'r')\n",
    "update_train_file = open('train-updated-single-label.csv', 'w')\n",
    "lines = train_file.readlines()\n",
    "# update_train_file.write(lines[0])\n",
    "for i in range(1, len(lines)):\n",
    "    classes = lines[i].rstrip().split(\":\")[0]\n",
    "    features = ''\n",
    "    true_classes = []\n",
    "    for j in range(len(classes)):\n",
    "        index = len(classes) -j - 1\n",
    "        if(classes[index] == ' '):\n",
    "            true_classes = list(map(int, classes[0:index].split(',')))\n",
    "            features = lines[i].split(classes[0:index])[1]\n",
    "            break\n",
    "#     for c in true_classes:\n",
    "#         if int(c) in relevant_classes_map:\n",
    "\n",
    "    new_classes = get_new_classes(true_classes)\n",
    "#     print(new_classes)\n",
    "#     print(features)\n",
    "    for c in new_classes:\n",
    "        update_train_file.write(c + features)\n",
    "#     break\n",
    "train_file.close()\n",
    "update_train_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "from sklearn.datasets import load_svmlight_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "need more than 1 value to unpack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-53e7d0acad81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_svmlight_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train-updated-single-label.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/tarique/Installations/anaconda3/lib/python3.5/site-packages/sklearn/datasets/svmlight_format.py\u001b[0m in \u001b[0;36mload_svmlight_file\u001b[0;34m(f, n_features, dtype, multilabel, zero_based, query_id)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \"\"\"\n\u001b[1;32m    132\u001b[0m     return tuple(load_svmlight_files([f], n_features, dtype, multilabel,\n\u001b[0;32m--> 133\u001b[0;31m                                      zero_based, query_id))\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tarique/Installations/anaconda3/lib/python3.5/site-packages/sklearn/datasets/svmlight_format.py\u001b[0m in \u001b[0;36mload_svmlight_files\u001b[0;34m(files, n_features, dtype, multilabel, zero_based, query_id)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \"\"\"\n\u001b[1;32m    249\u001b[0m     r = [_open_and_load(f, dtype, multilabel, bool(zero_based), bool(query_id))\n\u001b[0;32m--> 250\u001b[0;31m          for f in files]\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     if (zero_based is False\n",
      "\u001b[0;32m/home/tarique/Installations/anaconda3/lib/python3.5/site-packages/sklearn/datasets/svmlight_format.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \"\"\"\n\u001b[1;32m    249\u001b[0m     r = [_open_and_load(f, dtype, multilabel, bool(zero_based), bool(query_id))\n\u001b[0;32m--> 250\u001b[0;31m          for f in files]\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     if (zero_based is False\n",
      "\u001b[0;32m/home/tarique/Installations/anaconda3/lib/python3.5/site-packages/sklearn/datasets/svmlight_format.py\u001b[0m in \u001b[0;36m_open_and_load\u001b[0;34m(f, dtype, multilabel, zero_based, query_id)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_gen_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mactual_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0m_load_svmlight_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultilabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_based\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;31m# convert from array.array, give data the right dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32msklearn/datasets/_svmlight_format.pyx\u001b[0m in \u001b[0;36msklearn.datasets._svmlight_format._load_svmlight_file (sklearn/datasets/_svmlight_format.c:3020)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: need more than 1 value to unpack"
     ]
    }
   ],
   "source": [
    "dataf = load_svmlight_file(\"train-updated-single-label.csv\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
