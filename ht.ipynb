{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pylab as plt\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysing the graph of hierarcy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#visualize graph\n",
    "def save_graph(graph,file_name):\n",
    "    #initialze Figure\n",
    "    plt.figure(num=None, figsize=(20, 20), dpi=80)\n",
    "    plt.axis('off')\n",
    "    fig = plt.figure(1)\n",
    "    pos = nx.spring_layout(graph)\n",
    "    nx.draw_networkx_nodes(graph,pos)\n",
    "    nx.draw_networkx_edges(graph,pos)\n",
    "\n",
    "    cut = 1.00\n",
    "    xmax = cut * max(xx for xx, yy in pos.values())\n",
    "    ymax = cut * max(yy for xx, yy in pos.values())\n",
    "    plt.xlim(0, xmax)\n",
    "    plt.ylim(0, ymax)\n",
    "\n",
    "    plt.savefig(file_name,bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    del fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise is to remove a part of hierarchy to make data managable for my system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create directed graph from hierarchy\n",
    "def create_digraph(file):\n",
    "    graph = nx.DiGraph()\n",
    "\n",
    "    file = open(file)\n",
    "\n",
    "    lines = file.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        edge = line.rstrip().split(' ')\n",
    "        graph.add_edge(int(edge[0]), int(edge[1]))\n",
    "    file.close()\n",
    "    return graph\n",
    "\n",
    "graph = create_digraph('hierarchy.txt')\n",
    "all_nodes = nx.nodes(graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478020\n",
      "250000\n",
      "228020\n"
     ]
    }
   ],
   "source": [
    "print(len(all_nodes))\n",
    "nodes_to_remove = all_nodes[0:250000]\n",
    "print(len(nodes_to_remove))\n",
    "graph.remove_nodes_from(nodes_to_remove)\n",
    "print(graph.number_of_nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing some nodes from the graph we now look for largest connected component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147739\n"
     ]
    }
   ],
   "source": [
    "#find a connected component of graph with maximum number of nodes\n",
    "def single_connected_graph(graph):\n",
    "    new_graphs = nx.weakly_connected_component_subgraphs(graph)\n",
    "\n",
    "    the_graphs = []\n",
    "\n",
    "    for g in new_graphs:\n",
    "        the_graphs.append(g)\n",
    "    max_nodes = 0\n",
    "    max_nodes_index = 0\n",
    "    index = 0\n",
    "    for g in the_graphs:\n",
    "        num_nodes = g.number_of_nodes()\n",
    "        if(num_nodes > max_nodes):\n",
    "            max_nodes = num_nodes\n",
    "            max_nodes_index = index\n",
    "        index += 1\n",
    "    final_graph = the_graphs[max_nodes_index]\n",
    "    return final_graph\n",
    "\n",
    "final_graph = single_connected_graph(graph)\n",
    "final_nodes = nx.nodes(final_graph)\n",
    "print(len(final_nodes))\n",
    "# nx.write_edgelist(final_graph, \"new_hierarchy.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the nodes in a subset of heirarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330281\n",
      "[1048576, 1048577, 2, 174763, 4, 2097157, 6, 1, 8, 2097161]\n"
     ]
    }
   ],
   "source": [
    "discarded_nodes = []\n",
    "map_final_nodes = {}\n",
    "\n",
    "for node in final_nodes:\n",
    "    map_final_nodes[node] = 1\n",
    "\n",
    "for node in all_nodes:\n",
    "    if(node not in map_final_nodes):\n",
    "        discarded_nodes.append(node)\n",
    "    \n",
    "map_discarded_nodes = {}\n",
    "\n",
    "for node in discarded_nodes:\n",
    "    map_discarded_nodes[node] = 1\n",
    "print(len(discarded_nodes))\n",
    "print(discarded_nodes[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we filter our data to retain only the data with nodes in our retained hierarchy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = open('train-remapped.csv', 'r')\n",
    "output_file = open('final_training.csv', 'w')\n",
    "lines = data.readlines()\n",
    "output_file.write(lines[0])\n",
    "for i in range(1,len(lines)):\n",
    "#     print(lines[i])\n",
    "    classes = lines[i].rstrip().split(\":\")[0]\n",
    "    true_classes = []\n",
    "    for j in range(len(classes)):\n",
    "        index = len(classes) -j - 1\n",
    "        if(classes[index] == ' '):\n",
    "            true_classes = classes[0:index].split(',')\n",
    "            break\n",
    "    flag = False\n",
    "    for c in true_classes:\n",
    "        if int(c) in map_discarded_nodes:\n",
    "            flag = True\n",
    "            break\n",
    "    if(not flag):\n",
    "        output_file.write(lines[i])\n",
    "    \n",
    "    \n",
    "data.close()\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2365437\n",
      "216313\n"
     ]
    }
   ],
   "source": [
    "print(len(lines))\n",
    "\n",
    "output = open('final_training.csv', 'r')\n",
    "print(len(output.readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.is_directed_acyclic_graph(final_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that even the reduced hierarchy is not a DAG. This is problamatic for any meaningful traversal through the heirarchy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of looking at the data would be looking at most recurring classes and those should be the classes we may focus on first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#count number of examples for each class\n",
    "import operator\n",
    "def count_concepts(file_name):\n",
    "    file = open(file_name, 'r')\n",
    "    lines = file.readlines()\n",
    "    class_map ={}\n",
    "    for i in range(1, len(lines)):\n",
    "        classes = lines[i].rstrip().split(\":\")[0]\n",
    "        true_classes = []\n",
    "        for j in range(len(classes)):\n",
    "            index = len(classes) -j - 1\n",
    "            if(classes[index] == ' '):\n",
    "                true_classes = classes[0:index].split(',')\n",
    "                break\n",
    "        for c in true_classes:\n",
    "            if int(c) in class_map:\n",
    "                class_map[int(c)] = class_map[int(c)] + 1\n",
    "            else:\n",
    "                class_map[int(c)] = 1\n",
    "    file.close()\n",
    "    return class_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(24177, 387168), (285613, 41104), (98808, 14838), (264962, 12556), (167593, 11400), (242532, 10435), (52954, 10026), (300558, 9473), (444502, 9217), (78249, 9161)]\n",
      "[(445683, 1), (445690, 1), (445691, 1), (445699, 1), (445707, 1), (445710, 1), (445717, 1), (445722, 1), (445724, 1)]\n",
      "43368\n"
     ]
    }
   ],
   "source": [
    "class_map = count_concepts('train-remapped.csv')\n",
    "\n",
    "sorted_list = sorted(class_map.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "print(sorted_list[0:10])\n",
    "print(sorted_list[-10:-1])\n",
    "print(len(filter(lambda x: x[1] == 1, sorted_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some classes are dispropotionately popular while a lot (43368) have only 1 sample available. So we look for classes which have got some reasonable number of sample available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24177, 285613, 98808, 264962, 167593, 242532, 52954]\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "#Find all classes with atleast 1000 examples in dataset\n",
    "relevant_classes = list(map(lambda x: x[0], list(filter(lambda x: x[1] >= 10000, sorted_list))))\n",
    "print(relevant_classes)\n",
    "print(len(relevant_classes))\n",
    "\n",
    "relevant_classes_map = {}\n",
    "\n",
    "for c in relevant_classes:\n",
    "    relevant_classes_map[c] = 1\n",
    "# print(relevant_classes_map.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we consider classes which have atleast 10000 samples then we get onl 7 classes in case of atleast 1000 samples likewise there are around 350 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hfile = open('hierarchy.txt', 'r')\n",
    "newhfile = open('updated_hierarchy.txt', 'w')\n",
    "for line in hfile.readlines():\n",
    "    x = line.rstrip().split(' ')\n",
    "    \n",
    "    if(int(x[0]) in relevant_classes_map or int(x[1]) in relevant_classes_map):\n",
    "        newhfile.write(line)\n",
    "hfile.close()\n",
    "newhfile.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train_file = open('train-remapped.csv', 'r')\n",
    "# update_train_file = open('train-updated.csv', 'w')\n",
    "# lines = train_file.readlines()\n",
    "# # update_train_file.write(lines[0])\n",
    "# for i in range(1, len(lines)):\n",
    "#     classes = lines[i].rstrip().split(\":\")[0]\n",
    "#     true_classes = []\n",
    "#     for j in range(len(classes)):\n",
    "#         index = len(classes) -j - 1\n",
    "#         if(classes[index] == ' '):\n",
    "#             true_classes = classes[0:index].split(',')\n",
    "#             break\n",
    "# #     for c in true_classes:\n",
    "# #         if int(c) in relevant_classes_map:\n",
    "#     new_classes = get_new_classes(true_classese)\n",
    "#     if(len(true_classes) > 0):\n",
    "#         update_train_file.write(lines[i])\n",
    "# #     break\n",
    "# train_file.close()\n",
    "# update_train_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "723085\n"
     ]
    }
   ],
   "source": [
    "update_train_file = open('train-updated.csv', 'r')\n",
    "print(len(update_train_file.readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_new_classes(classes):\n",
    "    new_classes = []\n",
    "    for c in classes:\n",
    "        if c in relevant_classes_map:\n",
    "            new_classes.append(c)\n",
    "    return list(map(str, new_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train_file = open('train-remapped.csv', 'r')\n",
    "# update_train_file = open('train-updated-small.csv', 'w')\n",
    "# lines = train_file.readlines()\n",
    "# # update_train_file.write(lines[0])\n",
    "# for i in range(1, len(lines)):\n",
    "#     classes = lines[i].rstrip().split(\":\")[0]\n",
    "#     features = ''\n",
    "#     true_classes = []\n",
    "#     for j in range(len(classes)):\n",
    "#         index = len(classes) -j - 1\n",
    "#         if(classes[index] == ' '):\n",
    "#             true_classes = list(map(int, classes[0:index].split(',')))\n",
    "#             features = lines[i].split(classes[0:index])[1]\n",
    "#             break\n",
    "# #     for c in true_classes:\n",
    "# #         if int(c) in relevant_classes_map:\n",
    "\n",
    "#     new_classes = get_new_classes(true_classes)\n",
    "# #     print(new_classes)\n",
    "# #     print(features)\n",
    "#     if(len(new_classes) > 0):\n",
    "#         update_train_file.write(str(\",\".join(new_classes)) + features)\n",
    "# #     break\n",
    "# train_file.close()\n",
    "# update_train_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#This method is for measuring recall and  mistake(when a sample is assigned only one class and it is not among any valid class for the sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def multilabel_performance(pred, true, indexes):\n",
    "    \n",
    "    spliced_indexes  = []\n",
    "    previous_index = indexes[0]\n",
    "    temp_indexes = []\n",
    "    total_score = 0\n",
    "    total_mistake = 0\n",
    "    for i in range(len(indexes)):\n",
    "        if(indexes[i] != previous_index):\n",
    "            spliced_indexes.append(temp_indexes)\n",
    "            temp_indexes = []\n",
    "        temp_indexes.append(i)\n",
    "    spliced_indexes.append(temp_indexes)\n",
    "    total_samples = len(spliced_indexes)\n",
    "    for same_index  in spliced_indexes:\n",
    "        temp_score = 0\n",
    "        for x in same_index:\n",
    "            if(pred[x] == true[x]):\n",
    "                temp_score = 1\n",
    "        if(temp_score == 1):\n",
    "            total_score += 1\n",
    "            temp_score = 0\n",
    "        else:\n",
    "            total_mistake += 1\n",
    "    return float(total_score)/total_samples, float(total_mistake)/total_samples\n",
    "            \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Here we convert the data from multilabel to single label for easy manipulation. We also retain their indexes in case a sample falls in more than one class. These indexes will be required while measuring accuracy of prediction. When a sample has multiple labels we create a separate row for it with each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_file = open('train-remapped.csv', 'r')\n",
    "update_train_file = open('train-updated-single-label-small.csv', 'w')\n",
    "index_file = open('index.csv', 'w')\n",
    "lines = train_file.readlines()\n",
    "# update_train_file.write(lines[0])\n",
    "k = 0\n",
    "for i in range(1, len(lines)):\n",
    "    classes = lines[i].rstrip().split(\":\")[0]\n",
    "    features = ''\n",
    "    true_classes = []\n",
    "    for j in range(len(classes)):\n",
    "        index = len(classes) -j - 1\n",
    "        if(classes[index] == ' '):\n",
    "            true_classes = list(map(int, classes[0:index].split(',')))\n",
    "            features = lines[i].split(classes[0:index])[1]\n",
    "            break\n",
    "#     for c in true_classes:\n",
    "#         if int(c) in relevant_classes_map:\n",
    "\n",
    "    new_classes = get_new_classes(true_classes)\n",
    "#     print(new_classes)\n",
    "#     print(features)\n",
    "    flag = False\n",
    "    for c in new_classes:\n",
    "        update_train_file.write(c + features)\n",
    "        index_file.write(str(k) + '\\n')\n",
    "        flag = True\n",
    "    if(flag):\n",
    "        k += 1\n",
    "        flag = False\n",
    "        \n",
    "#     break\n",
    "train_file.close()\n",
    "update_train_file.close()\n",
    "index_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "import sklearn\n",
    "import mdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataf = load_svmlight_file(\"train-updated-single-label-small.csv\")\n",
    "indexes = []\n",
    "\n",
    "with open('index.csv', 'r') as f:\n",
    "    for x in f.readlines():\n",
    "        indexes.append(int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "487527\n"
     ]
    }
   ],
   "source": [
    "train_samples = 100000\n",
    "test_samples = 100000\n",
    "X_train = dataf[0][0:train_samples]\n",
    "y_train = dataf[1][0:train_samples]\n",
    "indexes_train = indexes[0:train_samples]\n",
    "X_test = dataf[0][train_samples:train_samples + test_samples]\n",
    "y_test = dataf[1][train_samples:train_samples + test_samples]\n",
    "indexes_test = indexes[train_samples:train_samples + test_samples]\n",
    "print(len(indexes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Testing SVM classification on a small sample and see how it performs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = sklearn.svm.SVC(verbose=3)\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[76790  1000  3739  1910  4811  5488  6262]\n",
      " [    0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0]]\n",
      "0.7679\n",
      "(0.7679, 0.2321)\n",
      "****************************************************************************************************\n",
      "[[78695  1904  2777  1864  2318  2634  9808]\n",
      " [    0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0]]\n",
      "0.78695\n",
      "(0.78695, 0.21305)\n"
     ]
    }
   ],
   "source": [
    "train_pred = model.predict(X_train)\n",
    "\n",
    "\n",
    "print(sklearn.metrics.confusion_matrix(train_pred, y_train))\n",
    "print(sklearn.metrics.accuracy_score(train_pred, y_train))\n",
    "print(multilabel_performance(train_pred, y_train, indexes_train))\n",
    "\n",
    "print(\"*\" * 100)\n",
    "test_pred = model.predict(X_test)\n",
    "\n",
    "print(sklearn.metrics.confusion_matrix(test_pred, y_test))\n",
    "print(sklearn.metrics.accuracy_score(test_pred, y_test))\n",
    "print(multilabel_performance(test_pred, y_test, indexes_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Using svm to train a model. The model is very simple it predicts every sample as the most popular class. In doing so it may reach around .787 recall. To do better lets use a simple decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            random_state=None, splitter='best')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeldecisionTree = sklearn.tree.DecisionTreeClassifier()\n",
    "modeldecisionTree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[76790   800     0     0  3880  3882  6261]\n",
      " [    0   200     0     0     0     0     0]\n",
      " [    0     0  3739  1425     0     0     0]\n",
      " [    0     0     0   485     0     0     0]\n",
      " [    0     0     0     0   931   835     0]\n",
      " [    0     0     0     0     0   771     0]\n",
      " [    0     0     0     0     0     0     1]]\n",
      "0.82917\n",
      "(0.82917, 0.17083)\n",
      "****************************************************************************************************\n",
      "[[77879  1717   339   224  2013  2037  9542]\n",
      " [   53   186     0     0     0     0     3]\n",
      " [  525     0  2266  1466    16    51   202]\n",
      " [   31     0   160   169     2     5     7]\n",
      " [  118     0     7     3   226   288    39]\n",
      " [   87     1     5     2    61   253    15]\n",
      " [    2     0     0     0     0     0     0]]\n",
      "0.80979\n",
      "(0.80979, 0.19021)\n"
     ]
    }
   ],
   "source": [
    "train_pred = modeldecisionTree.predict(X_train)\n",
    "print(sklearn.metrics.confusion_matrix(train_pred, y_train))\n",
    "print(sklearn.metrics.accuracy_score(train_pred, y_train))\n",
    "print(multilabel_performance(train_pred, y_train, indexes_train))\n",
    "\n",
    "\n",
    "print(\"*\" * 100)\n",
    "test_pred = modeldecisionTree.predict(X_test)\n",
    "print(sklearn.metrics.confusion_matrix(test_pred, y_test))\n",
    "print(sklearn.metrics.accuracy_score(test_pred, y_test))\n",
    "print(multilabel_performance(test_pred, y_test, indexes_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###With decision tree there is improvement. Shifting of prediction towards diagonal is observed. When base line is that good much improvement is difficult. There is not much overfitting too as train and test performance is close"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
